{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\nfrom typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport logging\nimport os\nimport gc\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\n# logging setting \n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-20T05:49:55.635307Z","iopub.execute_input":"2023-09-20T05:49:55.635687Z","iopub.status.idle":"2023-09-20T05:50:29.568069Z","shell.execute_reply.started":"2023-09-20T05:49:55.635657Z","shell.execute_reply":"2023-09-20T05:50:29.566934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seed\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \nseed_everything(seed=42)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-20T05:50:29.571049Z","iopub.execute_input":"2023-09-20T05:50:29.571342Z","iopub.status.idle":"2023-09-20T05:50:29.583873Z","shell.execute_reply.started":"2023-09-20T05:50:29.571303Z","shell.execute_reply":"2023-09-20T05:50:29.582882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name=\"deberta-v3-large-finetune\"\n    learning_rate=1e-5\n    weight_decay=1e-8\n    hidden_dropout_prob=0.\n    attention_probs_dropout_prob=0.\n    num_train_epochs=2\n    n_splits=4\n    batch_size=8\n    random_seed=42\n    save_steps=200\n    max_length=1600\n    folds=[0,1,2,3]\n    pl_dir='kaggle/input/pl/'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-20T05:50:29.585599Z","iopub.execute_input":"2023-09-20T05:50:29.586086Z","iopub.status.idle":"2023-09-20T05:50:29.592995Z","shell.execute_reply.started":"2023-09-20T05:50:29.586046Z","shell.execute_reply":"2023-09-20T05:50:29.591993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n\n\n# summaries_train = summaries_train.head(10) # for dev mode\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-20T05:50:29.596508Z","iopub.execute_input":"2023-09-20T05:50:29.597239Z","iopub.status.idle":"2023-09-20T05:50:29.729627Z","shell.execute_reply.started":"2023-09-20T05:50:29.597197Z","shell.execute_reply":"2023-09-20T05:50:29.728654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/commonlitexp03/deberta-v3-large-finetune/fold_0/\")\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = SpellChecker() #Speller(lang='en')\n        \n    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n        \"\"\" text length \"\"\"\n        tokenizer=self.tokenizer\n        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n\n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int):\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n\n        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n        # original_ngram_freq = Counter(ngrams(original_words, n))\n        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.speller.unknown(wordlist)))\n\n        return amount_miss\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(\n                self.tokenizer.encode(x), \n                skip_special_tokens=True\n            )\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(\n                self.tokenizer.encode(x), \n                skip_special_tokens=True\n            )\n\n        )\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n\n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        \n        # Crate dataframe with count of each category NERs overlap for all the summaries\n        # Because it spends too much time for this feature, I don't use this time.\n#         ners_count_df  = input_df.progress_apply(\n#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n#         ).fillna(0)\n#         self.ner_keys = ners_count_df.columns\n#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n#         # join ner count dataframe with train dataframe\n#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-20T05:50:29.731356Z","iopub.execute_input":"2023-09-20T05:50:29.7316Z","iopub.status.idle":"2023-09-20T05:50:31.462365Z","shell.execute_reply.started":"2023-09-20T05:50:29.731568Z","shell.execute_reply":"2023-09-20T05:50:31.461301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\ntest['length'] = test['summary_length'] + test['prompt_length']\ntest = test.sort_values('length', ascending=True).reset_index(drop=True)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:31.464271Z","iopub.execute_input":"2023-09-20T05:50:31.464639Z","iopub.status.idle":"2023-09-20T05:50:55.546806Z","shell.execute_reply.started":"2023-09-20T05:50:31.46459Z","shell.execute_reply":"2023-09-20T05:50:55.54578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:55.548799Z","iopub.execute_input":"2023-09-20T05:50:55.549623Z","iopub.status.idle":"2023-09-20T05:50:55.596117Z","shell.execute_reply.started":"2023-09-20T05:50:55.54958Z","shell.execute_reply":"2023-09-20T05:50:55.595084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:55.597593Z","iopub.execute_input":"2023-09-20T05:50:55.597932Z","iopub.status.idle":"2023-09-20T05:50:55.607521Z","shell.execute_reply.started":"2023-09-20T05:50:55.597892Z","shell.execute_reply":"2023-09-20T05:50:55.606407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor","metadata":{}},{"cell_type":"code","source":"class ScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                inputs: List[str],\n                target_cols: List[str],\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        \n        self.input_col = \"input\" # col name of model input after text concat sep token\n        \n        self.input_text_cols = inputs\n        self.target_cols = target_cols\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/commonlitexp03/deberta-v3-large-finetune/fold_0/\")\n        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/commonlitexp03/deberta-v3-large-finetune/fold_0\")\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 2,\n            \"problem_type\": \"regression\",\n        })\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n    def concatenate_with_sep_token(self, row):\n        sep = \" \" + self.tokenizer.sep_token + \" \"        \n        return sep.join(row[self.input_text_cols])\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[\"content\"], examples[\"wording\"]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                        padding=\"longest\",\n                        truncation=True,\n                        max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                        padding=\"longest\",\n                        truncation=True,\n                        max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        train_df[self.input_col] = train_df.apply(self.concatenate_with_sep_token, axis=1)\n        valid_df[self.input_col] = valid_df.apply(self.concatenate_with_sep_token, axis=1)        \n\n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        print(train_df.shape)\n        \n        model = AutoModelForSequenceClassification.from_config(\n           # f\"microsoft/deberta-v3-large\", \n            config=self.model_config\n        )\n        state = torch.load(f\"/kaggle/input/commonlitexp03/deberta-v3-large-finetune/fold_{fold}/pytorch_model.bin\",\n                           map_location=torch.device('cpu'))\n        model.load_state_dict(state)\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size, \n            per_device_eval_batch_size=batch_size,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            gradient_checkpointing=True,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"mcrmse\",\n            save_total_limit=1,\n            fp16=True,\n            auto_find_batch_size=True,\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_mcrmse,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        model.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        model.cpu()\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                batch_size: int,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        test_df[self.input_col] = test_df.apply(self.concatenate_with_sep_token, axis=1)\n\n        test_dataset = Dataset.from_pandas(test_df[[self.input_col]], preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model = AutoModelForSequenceClassification.from_pretrained(f\"/kaggle/input/commonlitexp03/deberta-v3-large-finetune/fold_{fold}/\")\n        model.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n\n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train=False,\n            do_predict=True,\n            per_device_eval_batch_size=batch_size,\n            dataloader_drop_last=False,\n            fp16=True,\n            auto_find_batch_size=True,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n        pred_df = pd.DataFrame(\n            preds, \n            columns=[\n                f\"content_pred\", \n                f\"wording_pred\"\n           ]\n        )\n        \n        model.cpu()\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return pred_df","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:58:56.387476Z","iopub.execute_input":"2023-09-20T05:58:56.387768Z","iopub.status.idle":"2023-09-20T05:58:56.418586Z","shell.execute_reply.started":"2023-09-20T05:58:56.387739Z","shell.execute_reply":"2023-09-20T05:58:56.41736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        targets: List[str],\n        inputs: List[str],\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        train_data = train_df[train_df[\"fold\"] != fold]\n        # train_data = pd.read_csv(CFG.pl_dir + f'pl_{fold}.csv')\n        print(train_data.shape)\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    mode: str,\n    targets: List[str],\n    inputs: List[str],\n    save_each_model: bool,\n    n_splits: int,\n    batch_size: int,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    \n    columns = list(train_df.columns.values)\n    \n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n        \n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred_df = csr.predict(\n            test_df=valid_data, \n            batch_size=batch_size,\n            fold=fold\n        )\n\n        train_df.loc[valid_data.index, f\"content_{mode}_pred\"] = pred_df[f\"content_pred\"].values\n        train_df.loc[valid_data.index, f\"wording_{mode}_pred\"] = pred_df[f\"wording_pred\"].values\n                \n    return train_df[columns + [f\"content_{mode}_pred\", f\"wording_{mode}_pred\"]]\n    \ndef predict(\n    test_df: pd.DataFrame,\n    mode: str,\n    targets:List[str],\n    inputs: List[str],\n    save_each_model: bool,\n    n_splits: int,\n    batch_size: int,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n    \n    columns = list(test_df.columns.values)\n\n    for fold in CFG.folds:\n        print(f\"fold {fold}:\")\n        \n        model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ScoreRegressor(\n            model_name=model_name,\n            target_cols=targets,\n            inputs= inputs,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred_df = csr.predict(\n            test_df=test_df, \n            batch_size=batch_size,\n            fold=fold\n        )\n\n        test_df[f\"content_{mode}_pred_{fold}\"] = pred_df[f\"content_pred\"].values\n        test_df[f\"wording_{mode}_pred_{fold}\"] = pred_df[f\"wording_pred\"].values\n\n    test_df[f\"content_{mode}_pred\"] = test_df[[f\"content_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n    test_df[f\"wording_{mode}_pred\"] = test_df[[f\"wording_{mode}_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1)\n    #test_df[f\"content_{mode}_pred\"] = test_df[f\"content_{mode}_pred_{fold}\"]# for fold in range(n_splits)]].mean(axis=1)\n    #test_df[f\"wording_{mode}_pred\"] = test_df[f\"wording_{mode}_pred_{fold}\"]# for fold in range(n_splits)]].mean(axis=1)\n  \n    return test_df[columns + [f\"content_{mode}_pred\", f\"wording_{mode}_pred\"]]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:01:25.70643Z","iopub.execute_input":"2023-09-20T06:01:25.70672Z","iopub.status.idle":"2023-09-20T06:01:25.730959Z","shell.execute_reply.started":"2023-09-20T06:01:25.706687Z","shell.execute_reply":"2023-09-20T06:01:25.729867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\"wording\", \"content\"]\nmode = \"multi\"\ninput_cols = [\"text\", \"prompt_question\", \"prompt_text\"]\nmodel_cfg = CFG\n\n\ntrain = pd.read_csv('/kaggle/input/commonlitexp03/deberta-v3-large-finetune/oof_df.csv')\n# set validate result\nfor target in [\"content\", \"wording\"]:\n    rmse = mean_squared_error(train[target], train[f\"{target}_{mode}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\ntest = predict(\n    test,\n    mode=mode,\n    targets=targets,\n    inputs=input_cols,\n    save_each_model=False,\n    batch_size=model_cfg.batch_size,\n    n_splits=CFG.n_splits,\n    model_name=model_cfg.model_name,\n    hidden_dropout_prob=model_cfg.hidden_dropout_prob,\n    attention_probs_dropout_prob=model_cfg.attention_probs_dropout_prob,\n    max_length=model_cfg.max_length\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:01:49.001117Z","iopub.execute_input":"2023-09-20T06:01:49.00157Z","iopub.status.idle":"2023-09-20T06:01:57.984993Z","shell.execute_reply.started":"2023-09-20T06:01:49.001524Z","shell.execute_reply":"2023-09-20T06:01:57.983994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:02:07.189438Z","iopub.execute_input":"2023-09-20T06:02:07.18994Z","iopub.status.idle":"2023-09-20T06:02:07.233863Z","shell.execute_reply.started":"2023-09-20T06:02:07.189859Z","shell.execute_reply":"2023-09-20T06:02:07.232702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM model","metadata":{}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\"\n               ] + targets","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:02:17.87542Z","iopub.execute_input":"2023-09-20T06:02:17.875687Z","iopub.status.idle":"2023-09-20T06:02:17.881679Z","shell.execute_reply.started":"2023-09-20T06:02:17.87566Z","shell.execute_reply":"2023-09-20T06:02:17.880755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n        params = {\n                  'boosting_type': 'gbdt',\n                  'random_state': 42,\n                  'objective': 'regression',\n                  'metric': 'rmse',\n                  'learning_rate': 0.05,\n                  }\n\n        evaluation_results = {}\n        model = lgb.train(params,\n                          num_boost_round=10000,\n                            #categorical_feature = categorical_features,\n                          valid_names=['train', 'valid'],\n                          train_set=dtrain,\n                          valid_sets=dval,\n                          callbacks=[\n                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n                               lgb.log_evaluation(100),\n                              lgb.callback.record_evaluation(evaluation_results)\n                            ],\n                          )\n        models.append(model)\n    \n    model_dict[target] = models\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:02:20.669162Z","iopub.execute_input":"2023-09-20T06:02:20.669444Z","iopub.status.idle":"2023-09-20T06:02:20.678948Z","shell.execute_reply.started":"2023-09-20T06:02:20.669415Z","shell.execute_reply":"2023-09-20T06:02:20.677838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"# cv\n\"\"\"rmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        # ilocで取り出す行を指定\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:57.557346Z","iopub.status.idle":"2023-09-20T05:50:57.558325Z","shell.execute_reply.started":"2023-09-20T05:50:57.558032Z","shell.execute_reply":"2023-09-20T05:50:57.558062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict ","metadata":{}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\"length\"\n               ]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:02:29.157075Z","iopub.execute_input":"2023-09-20T06:02:29.157353Z","iopub.status.idle":"2023-09-20T06:02:29.167075Z","shell.execute_reply.started":"2023-09-20T06:02:29.157324Z","shell.execute_reply":"2023-09-20T06:02:29.161718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        # ilocで取り出す行を指定\n        X_eval_cv = test.drop(columns=drop_columns)\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:57.562731Z","iopub.status.idle":"2023-09-20T05:50:57.56322Z","shell.execute_reply.started":"2023-09-20T05:50:57.562937Z","shell.execute_reply":"2023-09-20T05:50:57.562963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-20T05:50:57.565098Z","iopub.status.idle":"2023-09-20T05:50:57.566039Z","shell.execute_reply.started":"2023-09-20T05:50:57.565732Z","shell.execute_reply":"2023-09-20T05:50:57.56576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"test[targets] = test[['content_multi_pred','wording_multi_pred']].values\nsample_submission = sample_submission.drop(columns=targets).merge(test[['student_id'] + targets], on='student_id', how='left')\ndisplay(sample_submission.head())\nsample_submission[['student_id'] + targets].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:02:32.97249Z","iopub.execute_input":"2023-09-20T06:02:32.972784Z","iopub.status.idle":"2023-09-20T06:02:33.002616Z","shell.execute_reply.started":"2023-09-20T06:02:32.972753Z","shell.execute_reply":"2023-09-20T06:02:33.001464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}